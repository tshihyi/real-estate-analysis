{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tshihyi/real-estate-analysis/blob/main/land_info.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install selenium\n",
        "!apt-get update # to update ubuntu to correctly run apt install\n",
        "!apt install chromium-chromedriver\n",
        "!cp /usr/lib/chromium-browser/chromedriver /usr/bin\n",
        "\n",
        "!pip install pyspark"
      ],
      "metadata": {
        "id": "4J754l3jLLdu",
        "outputId": "f63a3455-f76c-49f5-e878-6d06e4254b7a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting selenium\n",
            "  Downloading selenium-4.3.0-py3-none-any.whl (981 kB)\n",
            "\u001b[K     |████████████████████████████████| 981 kB 10.3 MB/s \n",
            "\u001b[?25hCollecting trio~=0.17\n",
            "  Downloading trio-0.21.0-py3-none-any.whl (358 kB)\n",
            "\u001b[K     |████████████████████████████████| 358 kB 48.5 MB/s \n",
            "\u001b[?25hCollecting urllib3[secure,socks]~=1.26\n",
            "  Downloading urllib3-1.26.10-py2.py3-none-any.whl (139 kB)\n",
            "\u001b[K     |████████████████████████████████| 139 kB 56.3 MB/s \n",
            "\u001b[?25hCollecting trio-websocket~=0.9\n",
            "  Downloading trio_websocket-0.9.2-py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium) (21.4.0)\n",
            "Collecting outcome\n",
            "  Downloading outcome-1.2.0-py2.py3-none-any.whl (9.7 kB)\n",
            "Collecting sniffio\n",
            "  Downloading sniffio-1.2.0-py3-none-any.whl (10 kB)\n",
            "Collecting async-generator>=1.9\n",
            "  Downloading async_generator-1.10-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium) (2.10)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium) (2.4.0)\n",
            "Collecting wsproto>=0.14\n",
            "  Downloading wsproto-1.1.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from urllib3[secure,socks]~=1.26->selenium) (1.7.1)\n",
            "Collecting cryptography>=1.3.4\n",
            "  Downloading cryptography-37.0.4-cp36-abi3-manylinux_2_24_x86_64.whl (4.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.1 MB 46.2 MB/s \n",
            "\u001b[?25hCollecting pyOpenSSL>=0.14\n",
            "  Downloading pyOpenSSL-22.0.0-py2.py3-none-any.whl (55 kB)\n",
            "\u001b[K     |████████████████████████████████| 55 kB 3.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from urllib3[secure,socks]~=1.26->selenium) (2022.6.15)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography>=1.3.4->urllib3[secure,socks]~=1.26->selenium) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography>=1.3.4->urllib3[secure,socks]~=1.26->selenium) (2.21)\n",
            "Collecting h11<1,>=0.9.0\n",
            "  Downloading h11-0.13.0-py3-none-any.whl (58 kB)\n",
            "\u001b[K     |████████████████████████████████| 58 kB 2.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from h11<1,>=0.9.0->wsproto>=0.14->trio-websocket~=0.9->selenium) (4.1.1)\n",
            "Installing collected packages: sniffio, outcome, h11, cryptography, async-generator, wsproto, urllib3, trio, pyOpenSSL, trio-websocket, selenium\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "requests 2.23.0 requires urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1, but you have urllib3 1.26.10 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed async-generator-1.10 cryptography-37.0.4 h11-0.13.0 outcome-1.2.0 pyOpenSSL-22.0.0 selenium-4.3.0 sniffio-1.2.0 trio-0.21.0 trio-websocket-0.9.2 urllib3-1.26.10 wsproto-1.1.0\n",
            "Get:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease [1,581 B]\n",
            "Get:2 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "Ign:3 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:4 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:5 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Get:7 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Get:8 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n",
            "Get:9 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages [817 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Hit:11 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Hit:12 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "Get:13 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ Packages [85.6 kB]\n",
            "Hit:14 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Get:16 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2,901 kB]\n",
            "Get:17 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,527 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [1,105 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [3,333 kB]\n",
            "Get:20 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [1,063 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,304 kB]\n",
            "Get:22 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [2,075 kB]\n",
            "Get:23 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [1,063 kB]\n",
            "Fetched 16.5 MB in 7s (2,304 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following additional packages will be installed:\n",
            "  chromium-browser chromium-browser-l10n chromium-codecs-ffmpeg-extra\n",
            "Suggested packages:\n",
            "  webaccounts-chromium-extension unity-chromium-extension\n",
            "The following NEW packages will be installed:\n",
            "  chromium-browser chromium-browser-l10n chromium-chromedriver\n",
            "  chromium-codecs-ffmpeg-extra\n",
            "0 upgraded, 4 newly installed, 0 to remove and 71 not upgraded.\n",
            "Need to get 89.8 MB of archives.\n",
            "After this operation, 302 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-codecs-ffmpeg-extra amd64 101.0.4951.64-0ubuntu0.18.04.1 [1,142 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-browser amd64 101.0.4951.64-0ubuntu0.18.04.1 [78.5 MB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-browser-l10n all 101.0.4951.64-0ubuntu0.18.04.1 [4,980 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-chromedriver amd64 101.0.4951.64-0ubuntu0.18.04.1 [5,153 kB]\n",
            "Fetched 89.8 MB in 6s (15.3 MB/s)\n",
            "Selecting previously unselected package chromium-codecs-ffmpeg-extra.\n",
            "(Reading database ... 155653 files and directories currently installed.)\n",
            "Preparing to unpack .../chromium-codecs-ffmpeg-extra_101.0.4951.64-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-codecs-ffmpeg-extra (101.0.4951.64-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-browser.\n",
            "Preparing to unpack .../chromium-browser_101.0.4951.64-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-browser (101.0.4951.64-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-browser-l10n.\n",
            "Preparing to unpack .../chromium-browser-l10n_101.0.4951.64-0ubuntu0.18.04.1_all.deb ...\n",
            "Unpacking chromium-browser-l10n (101.0.4951.64-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-chromedriver.\n",
            "Preparing to unpack .../chromium-chromedriver_101.0.4951.64-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-chromedriver (101.0.4951.64-0ubuntu0.18.04.1) ...\n",
            "Setting up chromium-codecs-ffmpeg-extra (101.0.4951.64-0ubuntu0.18.04.1) ...\n",
            "Setting up chromium-browser (101.0.4951.64-0ubuntu0.18.04.1) ...\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/x-www-browser (x-www-browser) in auto mode\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/gnome-www-browser (gnome-www-browser) in auto mode\n",
            "Setting up chromium-chromedriver (101.0.4951.64-0ubuntu0.18.04.1) ...\n",
            "Setting up chromium-browser-l10n (101.0.4951.64-0ubuntu0.18.04.1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
            "Processing triggers for mime-support (3.60ubuntu1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.3) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n",
            "cp: '/usr/lib/chromium-browser/chromedriver' and '/usr/bin/chromedriver' are the same file\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.3.0.tar.gz (281.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 281.3 MB 27 kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9.5\n",
            "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
            "\u001b[K     |████████████████████████████████| 199 kB 29.1 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.3.0-py2.py3-none-any.whl size=281764026 sha256=4fc4ebd461fc93e86704ed9f0bafbd72a5b99a157f8a40e7e32a7f69fb3b3cef\n",
            "  Stored in directory: /root/.cache/pip/wheels/7a/8e/1b/f73a52650d2e5f337708d9f6a1750d451a7349a867f928b885\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.5 pyspark-3.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Web Crawler\n",
        "\n",
        "**TODO**\n",
        "- [ ] if download fail, retry it"
      ],
      "metadata": {
        "id": "Ffpa0YqB-Rco"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.insert(0,'/usr/lib/chromium-browser/chromedriver')\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "\n",
        "import time\n",
        "\n",
        "chrome_options = webdriver.ChromeOptions()\n",
        "chrome_options.add_argument('--headless')\n",
        "chrome_options.add_argument('--no-sandbox')\n",
        "chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "chrome_prefs = {\"download.default_directory\": './content/drive'}\n",
        "chrome_options.experimental_options[\"prefs\"] = chrome_prefs\n",
        "\n",
        "driver = webdriver.Chrome('chromedriver', options=chrome_options)\n",
        "url = \"https://plvr.land.moi.gov.tw/DownloadOpenData\"\n",
        "driver.get(url)\n",
        "\n",
        "try:\n",
        "  WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.ID,'ui-id-2'))).click()\n",
        "  WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH,\"//select[@id='historySeason_id']/option[@value='108S2']\"))).click()\n",
        "  WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH,\"//select[@id='fileFormatId']/option[@value='csv']\"))).click() #csv\n",
        "  WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH,\"//input[@id='downloadTypeId2']\"))).click() #進階\n",
        "  WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH,\"//input[@value='A_lvr_land_A']\"))).click() #台北\n",
        "  WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH,\"//input[@value='F_lvr_land_A']\"))).click() #新北\n",
        "  WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH,\"//input[@value='H_lvr_land_A']\"))).click() #桃園\n",
        "  WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH,\"//input[@value='B_lvr_land_A']\"))).click() #台中\n",
        "  WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH,\"//input[@value='E_lvr_land_A']\"))).click() #高雄\n",
        "  WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH,\"//input[@id='downloadBtnId']\"))).click()  #下載\n",
        "  print(\"Operation successful !\")\n",
        "  driver.save_screenshot('shot.png')\n",
        "  time.sleep(60)\n",
        "except Exception:\n",
        "  driver.quit()\n"
      ],
      "metadata": {
        "id": "W6bVxwLFRYIA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4cfd9cb-eeb6-4a4b-87e1-f6fe0d44324b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Operation successful !\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from zipfile import ZipFile\n",
        "from pyspark.sql import SparkSession, DataFrame\n",
        "from functools import reduce  # For Python 3.x\n",
        "\n",
        "zip_file = ZipFile('./content/drive/download.zip')\n",
        "\n",
        "df_list = [\n",
        "  pd.read_csv(zip_file.open('A_lvr_land_A.csv')),\n",
        "  pd.read_csv(zip_file.open('F_lvr_land_A.csv')),\n",
        "  pd.read_csv(zip_file.open('H_lvr_land_A.csv')),\n",
        "  pd.read_csv(zip_file.open('B_lvr_land_A.csv')),\n",
        "  pd.read_csv(zip_file.open('E_lvr_land_A.csv')) \n",
        "]"
      ],
      "metadata": {
        "id": "MRAVH34DsP64"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data clearning and transform"
      ],
      "metadata": {
        "id": "vdsXYpJ0-JKJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def covert_num(floor_name):\n",
        "  result = 0\n",
        "  if isinstance(floor_name, int):\n",
        "    return floor_name\n",
        "\n",
        "  if isinstance(floor_name, float):\n",
        "    result = int(floor_name)\n",
        "    return result\n",
        "\n",
        "  if(floor_name.endswith('層')):\n",
        "    floor_name = floor_name[:-1]\n",
        "  #List of words\n",
        "  num_list = {\"一\": 1, \"二\":2 , \"三\":3 , \"四\":4 , \"五\":5 , \"六\":6, \"七\":7 , \"八\":8 , \"九\":9 , \"十\":10 }\n",
        "  \n",
        "  for i in range (0, len(floor_name)):\n",
        "    for k in num_list:\n",
        "      if len(floor_name) == 1:\n",
        "          if (k == floor_name[i]):\n",
        "              result = num_list[k]\n",
        "      elif len(floor_name) == 2:\n",
        "          if (k == floor_name[0]):\n",
        "            result = num_list[k] + num_list[floor_name[1]]\n",
        "          else:\n",
        "            result = num_list[floor_name[0]] * 10\n",
        "      elif len(floor_name) == 3 :\n",
        "          if (k == floor_name[1]):\n",
        "            result = num_list[floor_name[0]] * 10 + num_list[floor_name[2]]\n",
        "          else:\n",
        "            result = 0\n",
        "    return result\n",
        "\n",
        "def convert_western_date(txn_date):\n",
        "  if len(txn_date)>0:\n",
        "    txn_date = txn_date.replace(txn_date[0:3],str(int(txn_date[0:3])+1911))\n",
        "    txn_date = txn_date[0:4]+ \"-\" + txn_date[4:6] + \"-\" + txn_date[6:8]\n",
        "  return txn_date\n",
        "\n",
        "def data_cleaning(data_set, cityName):\n",
        "  data_set = data_set.drop([0])\n",
        "  data_set = data_set.fillna(0)\n",
        "  data_set['floor_Num'] = data_set['總樓層數'].apply(covert_num)\n",
        "  data_set['floor_Num'] = data_set['floor_Num'].astype(int)\n",
        "  data_set['主建物面積'] = data_set['主建物面積'].apply(str)\n",
        "  data_set['附屬建物面積'] = data_set['附屬建物面積'].apply(str)\n",
        "  data_set['陽台面積'] = data_set['陽台面積'].apply(str)\n",
        "  data_set['車位類別'] = data_set['車位類別'].apply(str)\n",
        "  data_set['移轉層次'] = data_set['移轉層次'].apply(str)\n",
        "  data_set['總樓層數'] = data_set['總樓層數'].apply(str)\n",
        "  data_set['主要用途'] = data_set['主要用途'].apply(str)\n",
        "  data_set['主要建材'] = data_set['主要建材'].apply(str)\n",
        "  data_set['建築完成年月'] = data_set['建築完成年月'].apply(str)\n",
        "  data_set['備註'] = data_set['備註'].apply(str)\n",
        "  data_set['單價元平方公尺'] = data_set['單價元平方公尺'].apply(str)\n",
        "  data_set['都市土地使用分區'] = data_set['都市土地使用分區'].apply(str)\n",
        "  data_set['非都市土地使用分區'] = data_set['非都市土地使用分區'].apply(str)\n",
        "  data_set['非都市土地使用編定'] = data_set['非都市土地使用編定'].apply(str)\n",
        "  data_set['交易年月日'] = data_set['交易年月日'].apply(convert_western_date)\n",
        "  data_set.insert(0, 'city', cityName)\n",
        "  return data_set\n",
        "\n",
        "df = pd.concat([data_cleaning(df_list[0], \"台北市\"),\n",
        "                data_cleaning(df_list[1], \"新北市\"),\n",
        "                data_cleaning(df_list[2], \"桃園市\"),\n",
        "                data_cleaning(df_list[3], \"台中市\"),\n",
        "                data_cleaning(df_list[4], \"高雄市\")], \n",
        "                axis = 0, \n",
        "                join=\"outer\",\n",
        "                ignore_index=True)\n"
      ],
      "metadata": {
        "id": "fJLQ51QwpmJh"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "dataFrame save to SQLite3 for creating RESTful API"
      ],
      "metadata": {
        "id": "8DuHGRYOtGdE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sqlite3\n",
        "\n",
        "#create db\n",
        "conn = sqlite3.connect('land.db')  \n",
        "cursor = conn.cursor()\n",
        "conn.commit()\n",
        "\n",
        "#replace: Drop the table before inserting new values.\n",
        "df.to_sql('land_txn_log', conn, if_exists='replace', index=False)\n",
        "us_df = pd.read_sql(\"SELECT count(*) FROM land_txn_log;\", conn)\n",
        "print(us_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I9HKaPwIs8xG",
        "outputId": "22a58dad-060b-452d-f6bc-4b72966508b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   count(*)\n",
            "0     49642\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convert to Spark DataFrame is working"
      ],
      "metadata": {
        "id": "Qg_hKUnavF84"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Create PySpark SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "            .master(\"local[1]\") \\\n",
        "            .appName(\"SparkMergeDataSet\") \\\n",
        "            .getOrCreate()\n",
        "\n",
        "#Convert Pandas DataFrame to Spark DataFrame\n",
        "sparkdf_list = [\n",
        "  spark.createDataFrame(data_cleaning(df_list[0], \"台北市\")),\n",
        "  spark.createDataFrame(data_cleaning(df_list[1], \"新北市\")),\n",
        "  spark.createDataFrame(data_cleaning(df_list[2], \"桃園市\")),\n",
        "  spark.createDataFrame(data_cleaning(df_list[3], \"台中市\")),\n",
        "  spark.createDataFrame(data_cleaning(df_list[4], \"高雄市\"))\n",
        "]\n",
        " \n",
        "spark_df = reduce(DataFrame.unionAll, sparkdf_list)\n",
        "\n",
        "print(\"total data count: \", spark_df.count())"
      ],
      "metadata": {
        "id": "tBPTPzKTvA5S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3efb0fa-153d-4178-a8d9-03001212c394"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total data count:  49642\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Merge Dataframes by Pyspark\n",
        "- Filter data by conditions\n",
        "- Generate JSON files"
      ],
      "metadata": {
        "id": "ECp6HSKtEnGo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from pyspark.sql.functions import desc\n",
        "\n",
        "query_df = spark_df.where('`主要用途` == \"住家用\"')\\\n",
        "                  .where('`建物型態` like \"住宅大樓%\"')\\\n",
        "                  .where('floor_num >= 13')\\\n",
        "                  .sort(desc(\"交易年月日\"))\n",
        "\n",
        "query_df = query_df.drop(\"floor_num\")\n",
        "pandas_df = query_df.toPandas()\n",
        "records = pandas_df.values.tolist()\n",
        "\n",
        "table = {}\n",
        "for column in records:\n",
        "  city = column[0]\n",
        "  district = column[1]\n",
        "  date = column[8]\n",
        "  building_state = column[2]\n",
        "  purpose = column[13]\n",
        "  floors = column[12]\n",
        "    \n",
        "  if not city in table.keys(): \n",
        "    table[city] = {}\n",
        "  if not date in table[city].keys():\n",
        "    table[city][date] = []\n",
        "  table[city][date].append({\n",
        "      \"鄉鎮市區\": district,\n",
        "      \"建物型態\": building_state,\n",
        "      \"主要用途\": purpose,\n",
        "      \"總樓層數\": floors\n",
        "  })\n",
        "\n",
        "result = []\n",
        "\n",
        "for city, date_table in table.items():\n",
        "  time_slots = []\n",
        "  for date, events in date_table.items():\n",
        "    time_slots.append({\n",
        "        \"date\": date,\n",
        "        \"events\": events\n",
        "  })\n",
        "      \n",
        "  result.append({\n",
        "      \"city\": city,\n",
        "      \"time_slots\": time_slots\n",
        "  })\n",
        "\n",
        "import io, json\n",
        "with io.open('result-part1.json', 'w', encoding='utf-8') as f:\n",
        "  for item in result[:2]:\n",
        "    f.write(json.dumps(item, ensure_ascii=False, indent=2))\n",
        "\n",
        "with io.open('result-part2.json', 'w', encoding='utf-8') as f:\n",
        "  for item in result[2:]:\n",
        "    f.write(json.dumps(item, ensure_ascii=False))"
      ],
      "metadata": {
        "id": "JI9J2uZD9-U6"
      },
      "execution_count": 46,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "land_info.ipynb",
      "provenance": [],
      "mount_file_id": "1yRxhUCezlJBQxfBA7eawGVwCXXx7sL8D",
      "authorship_tag": "ABX9TyNbgIOKCI8J1hqGmb8teWSN",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}