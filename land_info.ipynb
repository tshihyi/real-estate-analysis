{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4J754l3jLLdu"
      },
      "outputs": [],
      "source": [
        "!pip install selenium\n",
        "!apt-get update  # to update ubuntu to correctly run apt install\n",
        "!apt install chromium-chromedriver\n",
        "!cp / usr/lib/chromium-browser/chromedriver / usr/bin\n",
        "\n",
        "!pip install pyspark\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ffpa0YqB-Rco"
      },
      "source": [
        "Web Crawler\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W6bVxwLFRYIA"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from selenium import webdriver\n",
        "import sys\n",
        "sys.path.insert(0, '/usr/lib/chromium-browser/chromedriver')\n",
        "\n",
        "\n",
        "chrome_options = webdriver.ChromeOptions()\n",
        "chrome_options.add_argument('--headless')\n",
        "chrome_options.add_argument('--no-sandbox')\n",
        "chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "chrome_prefs = {\"download.default_directory\": './content/drive'}\n",
        "chrome_options.experimental_options[\"prefs\"] = chrome_prefs\n",
        "driver = webdriver.Chrome('chromedriver', options=chrome_options)\n",
        "url = \"https://plvr.land.moi.gov.tw/DownloadOpenData\"\n",
        "\n",
        "\n",
        "xpath_list = [\n",
        "    \"//select[@id='historySeason_id']/option[@value='108S2']\",\n",
        "    \"//select[@id='fileFormatId']/option[@value='csv']\",  # csv\n",
        "    \"//input[@id='downloadTypeId2']\",  # 進階\n",
        "    \"//input[@value='A_lvr_land_A']\",  # 台北\n",
        "    \"//input[@value='F_lvr_land_A']\",  # 新北\n",
        "    \"//input[@value='H_lvr_land_A']\",  # 桃園\n",
        "    \"//input[@value='B_lvr_land_A']\",  # 台中\n",
        "    \"//input[@value='E_lvr_land_A']\",  # 高雄\n",
        "    \"//input[@id='downloadBtnId']\"  # 下載\n",
        "]\n",
        "\n",
        "\n",
        "def clawer():\n",
        "    driver.get(url)\n",
        "    WebDriverWait(driver, 20).until(\n",
        "        EC.element_to_be_clickable((By.ID, 'ui-id-2'))).click()\n",
        "    for xpath in xpath_list:\n",
        "        WebDriverWait(driver, 20).until(\n",
        "            EC.element_to_be_clickable((By.XPATH, xpath))).click()\n",
        "    print(\"Operation successful !\")\n",
        "    time.sleep(60)\n",
        "\n",
        "\n",
        "try:\n",
        "    clawer()\n",
        "except Exception:\n",
        "    driver.quit()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MRAVH34DsP64"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from zipfile import ZipFile\n",
        "from pyspark.sql import SparkSession, DataFrame\n",
        "from functools import reduce  # For Python 3.x\n",
        "\n",
        "zip_file = ZipFile('./content/drive/download.zip')\n",
        "\n",
        "df_list = [\n",
        "    pd.read_csv(zip_file.open('A_lvr_land_A.csv')),\n",
        "    pd.read_csv(zip_file.open('F_lvr_land_A.csv')),\n",
        "    pd.read_csv(zip_file.open('H_lvr_land_A.csv')),\n",
        "    pd.read_csv(zip_file.open('B_lvr_land_A.csv')),\n",
        "    pd.read_csv(zip_file.open('E_lvr_land_A.csv'))\n",
        "]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vdsXYpJ0-JKJ"
      },
      "source": [
        "Data clean and transform\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 172,
      "metadata": {
        "id": "fJLQ51QwpmJh"
      },
      "outputs": [],
      "source": [
        "def covert_num(floor_name):\n",
        "    result = 0\n",
        "    if isinstance(floor_name, int):\n",
        "        return floor_name\n",
        "\n",
        "    if isinstance(floor_name, float):\n",
        "        result = int(floor_name)\n",
        "        return result\n",
        "\n",
        "    if(floor_name.endswith('層')):\n",
        "        floor_name = floor_name[:-1]\n",
        "    # List of words\n",
        "    num_list = {\"一\": 1, \"二\": 2, \"三\": 3, \"四\": 4, \"五\": 5,\n",
        "                \"六\": 6, \"七\": 7, \"八\": 8, \"九\": 9, \"十\": 10}\n",
        "\n",
        "    for i in range(0, len(floor_name)):\n",
        "        for k in num_list:\n",
        "            if len(floor_name) == 1:\n",
        "                if (k == floor_name[i]):\n",
        "                    result = num_list[k]\n",
        "            elif len(floor_name) == 2:\n",
        "                if (k == floor_name[0]):\n",
        "                    result = num_list[k] + num_list[floor_name[1]]\n",
        "                else:\n",
        "                    result = num_list[floor_name[0]] * 10\n",
        "            elif len(floor_name) == 3:\n",
        "                if (k == floor_name[1]):\n",
        "                    result = num_list[floor_name[0]] * \\\n",
        "                        10 + num_list[floor_name[2]]\n",
        "                else:\n",
        "                    result = 0\n",
        "        return result\n",
        "\n",
        "\n",
        "def convert_western_date(date):\n",
        "    if len(date) > 0:\n",
        "        date = date.replace(\n",
        "            date[0:3], str(int(date[0:3])+1911))\n",
        "        date = date[0:4] + \"-\" + date[4:6] + \"-\" + date[6:8]\n",
        "    return date\n",
        "\n",
        "\n",
        "def clean_data(data_set, cityName):\n",
        "    trans_dict = {\n",
        "        '主建物面積': str,\n",
        "        '附屬建物面積': str,\n",
        "        '陽台面積': str,\n",
        "        '車位類別': str,\n",
        "        '移轉層次': str,\n",
        "        '總樓層數': str,\n",
        "        '主要用途': str,\n",
        "        '主要建材': str,\n",
        "        '建築完成年月': str,\n",
        "        '備註': str,\n",
        "        '單價元平方公尺': str,\n",
        "        '都市土地使用分區': str,\n",
        "        '非都市土地使用分區': str,\n",
        "        '非都市土地使用編定': str\n",
        "    }\n",
        "\n",
        "    data_set = data_set.drop([0])\n",
        "    data_set = data_set.fillna(0)\n",
        "\n",
        "    # convert dataype to string or integer\n",
        "    for item in trans_dict.keys():\n",
        "        data_set[item] = data_set[item].astype(trans_dict[item])\n",
        "\n",
        "    # special processing\n",
        "    data_set['交易年月日'] = data_set['交易年月日'].apply(convert_western_date)\n",
        "    data_set['floor_Num'] = data_set['總樓層數'].apply(covert_num)\n",
        "    data_set.insert(0, 'city', cityName)\n",
        "\n",
        "    return data_set\n",
        "\n",
        "\n",
        "df = pd.concat([\n",
        "    clean_data(df_list[0], \"台北市\"),\n",
        "    clean_data(df_list[1], \"新北市\"),\n",
        "    clean_data(df_list[2], \"桃園市\"),\n",
        "    clean_data(df_list[3], \"台中市\"),\n",
        "    clean_data(df_list[4], \"高雄市\")\n",
        "],\n",
        "    axis=0,\n",
        "    join=\"outer\",\n",
        "    ignore_index=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qg_hKUnavF84"
      },
      "source": [
        "Convert to Spark DataFrame\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tBPTPzKTvA5S"
      },
      "outputs": [],
      "source": [
        "# Create PySpark SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .master(\"local[1]\") \\\n",
        "    .appName(\"SparkMergeDataSet\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Convert Pandas DataFrame to Spark DataFrame\n",
        "sparkdf_list = [\n",
        "    spark.createDataFrame(clean_data(df_list[0], \"台北市\")),\n",
        "    spark.createDataFrame(clean_data(df_list[1], \"新北市\")),\n",
        "    spark.createDataFrame(clean_data(df_list[2], \"桃園市\")),\n",
        "    spark.createDataFrame(clean_data(df_list[3], \"台中市\")),\n",
        "    spark.createDataFrame(clean_data(df_list[4], \"高雄市\"))\n",
        "]\n",
        "\n",
        "spark_df = reduce(DataFrame.unionAll, sparkdf_list)\n",
        "\n",
        "print(\"total data count: \", spark_df.count())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ECp6HSKtEnGo"
      },
      "source": [
        "- Merge Dataframes by Pyspark\n",
        "- Filter data by conditions\n",
        "- Generate JSON files\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 212,
      "metadata": {
        "id": "ySrk86oD-62-"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import desc, collect_list, struct, col, to_json\n",
        "import io\n",
        "import json\n",
        "\n",
        "filiter_df = spark_df.where('`主要用途` == \"住家用\"')\\\n",
        "                     .where('`建物型態` like \"住宅大樓%\"')\\\n",
        "                     .where('floor_num >= 13')\\\n",
        "                     .sort(desc(\"交易年月日\"))\n",
        "\n",
        "result_struct = struct(col('鄉鎮市區'),\n",
        "                       col('建物型態'),\n",
        "                       col('主要用途'),\n",
        "                       col('總樓層數'))\n",
        "\n",
        "result = filiter_df\\\n",
        "    .groupBy(['city', '交易年月日'])\\\n",
        "    .agg(collect_list(result_struct).alias('events'))\\\n",
        "    .groupBy(['city'])\\\n",
        "    .agg(collect_list(struct(col('交易年月日'), col('events'))).alias('time_slots'))\\\n",
        "    .toJSON()\\\n",
        "    .collect()\n",
        "\n",
        "with io.open('result-part1.json', 'w', encoding='utf-8') as f:\n",
        "    for item in result[:2]:\n",
        "        f.write(item)\n",
        "\n",
        "with io.open('result-part2.json', 'w', encoding='utf-8') as f:\n",
        "    for item in result[2:]:\n",
        "        f.write(item)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8DuHGRYOtGdE"
      },
      "source": [
        "dataFrame save to SQLite3 for creating RESTful API\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I9HKaPwIs8xG"
      },
      "outputs": [],
      "source": [
        "import sqlite3\n",
        "\n",
        "# create db\n",
        "conn = sqlite3.connect('land.db')\n",
        "cursor = conn.cursor()\n",
        "conn.commit()\n",
        "\n",
        "# replace: Drop the table before inserting new values.\n",
        "df.to_sql('land_txn_log', conn, if_exists='replace', index=False)\n",
        "us_df = pd.read_sql(\"SELECT count(*) FROM land_txn_log;\", conn)\n",
        "print(us_df)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "land_info.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
