{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4J754l3jLLdu"
      },
      "outputs": [],
      "source": [
        "!pip install selenium\n",
        "!apt-get update  # to update ubuntu to correctly run apt install\n",
        "!apt install chromium-chromedriver\n",
        "!cp / usr/lib/chromium-browser/chromedriver / usr/bin\n",
        "\n",
        "!pip install pyspark\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ffpa0YqB-Rco"
      },
      "source": [
        "Web Crawler\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W6bVxwLFRYIA"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from selenium import webdriver\n",
        "import sys\n",
        "sys.path.insert(0, '/usr/lib/chromium-browser/chromedriver')\n",
        "\n",
        "\n",
        "chrome_options = webdriver.ChromeOptions()\n",
        "chrome_options.add_argument('--headless')\n",
        "chrome_options.add_argument('--no-sandbox')\n",
        "chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "chrome_prefs = {\"download.default_directory\": './content/drive'}\n",
        "chrome_options.experimental_options[\"prefs\"] = chrome_prefs\n",
        "driver = webdriver.Chrome('chromedriver', options=chrome_options)\n",
        "url = \"https://plvr.land.moi.gov.tw/DownloadOpenData\"\n",
        "\n",
        "\n",
        "xpath_list = [\n",
        "    \"//select[@id='historySeason_id']/option[@value='108S2']\",\n",
        "    \"//select[@id='fileFormatId']/option[@value='csv']\",  # csv\n",
        "    \"//input[@id='downloadTypeId2']\",  # 進階\n",
        "    \"//input[@value='A_lvr_land_A']\",  # 台北\n",
        "    \"//input[@value='F_lvr_land_A']\",  # 新北\n",
        "    \"//input[@value='H_lvr_land_A']\",  # 桃園\n",
        "    \"//input[@value='B_lvr_land_A']\",  # 台中\n",
        "    \"//input[@value='E_lvr_land_A']\",  # 高雄\n",
        "    \"//input[@id='downloadBtnId']\"  # 下載\n",
        "]\n",
        "\n",
        "\n",
        "def clawer():\n",
        "    driver.get(url)\n",
        "    WebDriverWait(driver, 20).until(\n",
        "        EC.element_to_be_clickable((By.ID, 'ui-id-2'))).click()\n",
        "    for xpath in xpath_list:\n",
        "        WebDriverWait(driver, 20).until(\n",
        "            EC.element_to_be_clickable((By.XPATH, xpath))).click()\n",
        "    print(\"Operation successful !\")\n",
        "    time.sleep(60)\n",
        "\n",
        "\n",
        "try:\n",
        "    clawer()\n",
        "except Exception:\n",
        "    driver.quit()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vdsXYpJ0-JKJ"
      },
      "source": [
        "Data clean and transform\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "6Of-X0XC56Pa"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import desc, collect_list, struct, col, to_json, when, regexp_replace, translate, udf\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
        "from pyspark.sql import SparkSession, DataFrame\n",
        "from functools import reduce\n",
        "import zipfile\n",
        "import io\n",
        "\n",
        "spark = SparkSession.builder\\\n",
        "    .master(\"local[1]\")\\\n",
        "    .appName(\"SparkReadCSVExample\")\\\n",
        "    .getOrCreate()\n",
        "\n",
        "zip_path = './content/drive/download.zip'\n",
        "unzip_path = './content/files'\n",
        "\n",
        "zip_ref = zipfile.ZipFile(zip_path, 'r')\n",
        "zip_ref.extractall(unzip_path)\n",
        "zip_ref.close()\n",
        "\n",
        "file_dict = {\n",
        "    spark.read.options(header='True', inferSchema='True')\n",
        "    .csv(unzip_path+'/A_lvr_land_A.csv'): '臺北市',\n",
        "    spark.read.options(header='True', inferSchema='True')\n",
        "    .csv(unzip_path+'/F_lvr_land_A.csv'): '新北市',\n",
        "    spark.read.options(header='True', inferSchema='True')\n",
        "    .csv(unzip_path+'/H_lvr_land_A.csv'): '桃園市',\n",
        "    spark.read.options(header='True', inferSchema='True')\n",
        "    .csv(unzip_path+'/B_lvr_land_A.csv'): '台中市',\n",
        "    spark.read.options(header='True', inferSchema='True')\n",
        "    .csv(unzip_path+'/E_lvr_land_A.csv'): '高雄市'\n",
        "}\n",
        "\n",
        "# add city column\n",
        "df_list = []\n",
        "for item in file_dict:\n",
        "    df_list.append(\n",
        "        item.withColumn('city',\n",
        "             when((item.土地位置建物門牌[0:3] == file_dict[item]), item.土地位置建物門牌[0:3])\n",
        "            .otherwise(file_dict[item]))\n",
        "    )\n",
        "\n",
        "# marge dataframe by spark\n",
        "spark_df = reduce(DataFrame.unionAll, df_list)\n",
        "\n",
        "# remove english row\n",
        "spark_df = spark_df.where('`鄉鎮市區` not like \"The villages%\"')\n",
        "\n",
        "spark_df = spark_df.withColumn('floor_num',\n",
        "                    when(spark_df.總樓層數.endswith('層'),\n",
        "                    regexp_replace(spark_df.總樓層數, '層', ''))\n",
        "                   .otherwise('')\n",
        "           )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "dZs1VowePrLK"
      },
      "outputs": [],
      "source": [
        "def covert_num(floor_name):\n",
        "    result = 0\n",
        "    if isinstance(floor_name, int):\n",
        "        return floor_name\n",
        "\n",
        "    if isinstance(floor_name, float):\n",
        "        result = int(floor_name)\n",
        "        return result\n",
        "\n",
        "    # List of words\n",
        "    num_list = {\"一\": 1, \"二\": 2, \"三\": 3, \"四\": 4, \"五\": 5,\n",
        "                \"六\": 6, \"七\": 7, \"八\": 8, \"九\": 9, \"十\": 10}\n",
        "\n",
        "    for i in range(0, len(floor_name)):\n",
        "        for k in num_list:\n",
        "            if len(floor_name) == 1:\n",
        "                if (k == floor_name[i]):\n",
        "                    result = num_list[k]\n",
        "            elif len(floor_name) == 2:\n",
        "                if (k == floor_name[0]):\n",
        "                    result = num_list[k] + num_list[floor_name[1]]\n",
        "                else:\n",
        "                    result = num_list[floor_name[0]] * 10\n",
        "            elif len(floor_name) == 3:\n",
        "                if (k == floor_name[1]):\n",
        "                    result = num_list[floor_name[0]] * \\\n",
        "                        10 + num_list[floor_name[2]]\n",
        "                else:\n",
        "                    result = 0\n",
        "        return result\n",
        "\n",
        "def convert_western_date(date):\n",
        "    if len(date) > 0:\n",
        "        date = date.replace( \n",
        "            date[0:3], str(int(date[0:3])+1911))\n",
        "        date = date[0:4] + \"-\" + date[4:6] + \"-\" + date[6:8]\n",
        "    return date\n",
        "\n",
        "num_udf = udf(covert_num, IntegerType())\n",
        "western_date_udf = udf(convert_western_date, StringType())\n",
        "\n",
        "spark_df = spark_df.withColumn(\"floor_num\", num_udf(col(\"floor_num\")))\n",
        "spark_df = spark_df.withColumn(\"交易年月日\", western_date_udf(col(\"交易年月日\")))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "D2APdawfckC8"
      },
      "outputs": [],
      "source": [
        "filiter_df = spark_df.where('`主要用途` == \"住家用\"')\\\n",
        "                     .where('`建物型態` like \"住宅大樓%\"')\\\n",
        "                     .where('floor_num >= 13')\n",
        "\n",
        "result_struct = struct(col('鄉鎮市區').alias('district'),\n",
        "                       col('建物型態').alias('building_state'))\n",
        "\n",
        "result = filiter_df\\\n",
        "    .groupBy(['city', '交易年月日'])\\\n",
        "    .agg(collect_list(result_struct).alias('events'))\\\n",
        "    .sort(desc('交易年月日'))\\\n",
        "    .groupBy(['city'])\\\n",
        "    .agg(collect_list(struct(col('交易年月日').alias('date'), col('events'))).alias('time_slots'))\\\n",
        "    .toJSON()\\\n",
        "    .collect()\n",
        "\n",
        "with io.open('result-part1.json', 'w', encoding='utf-8') as f:\n",
        "    for item in result[:2]:\n",
        "        f.write(item + \"\\n\")\n",
        "\n",
        "with io.open('result-part2.json', 'w', encoding='utf-8') as f:\n",
        "    for item in result[2:]:\n",
        "        f.write(item + \"\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8DuHGRYOtGdE"
      },
      "source": [
        "dataFrame save to SQLite3 for creating RESTful API\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I9HKaPwIs8xG"
      },
      "outputs": [],
      "source": [
        "import sqlite3\n",
        "import pandas as pd\n",
        "\n",
        "# create db\n",
        "conn = sqlite3.connect('land.db')\n",
        "cursor = conn.cursor()\n",
        "conn.commit()\n",
        "\n",
        "df = spark_df.toPandas()\n",
        "# replace: Drop the table before inserting new values.\n",
        "df.to_sql('land_txn_log', conn, if_exists='replace', index=False)\n",
        "us_df = pd.read_sql(\"SELECT count(*) FROM land_txn_log;\", conn)\n",
        "print(us_df)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "land_info_clean_data_spark.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "interpreter": {
      "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
    },
    "kernelspec": {
      "display_name": "Python 3.8.9 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
